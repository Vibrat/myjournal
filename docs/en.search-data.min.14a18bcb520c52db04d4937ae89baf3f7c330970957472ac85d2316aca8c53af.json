[{"id":0,"href":"/docs/introduction/","title":"Introduction","section":"Docs","content":"Introduction #  Welcome, folks!\nI hope you\u0026rsquo;ll find something interesting.\nHere is what i write about:\n Math: mostly about subjects to satisfy my curiosity. Python: this is my job, i use python to make money as a software engineer. There will be two parts:  One for building software (protocal, asyncio, algorithm, design patterns, \u0026hellip;) One for data science (with libraries such as numpy, pandas, \u0026hellip;)   And bunch of other interesting stuffs.  All i just taught myself for years (along with professional working), so there are many parts left undone. Please feel free to hit me up via lamnguyen10@acm.org\n"},{"id":1,"href":"/docs/math/","title":"Math","section":"Docs","content":"Table of content #   Derivative of matrix  "},{"id":2,"href":"/docs/python/","title":"Python","section":"Docs","content":"Table of content #   Basic matrix operation in numpy  "},{"id":3,"href":"/docs/contact/","title":"Contact","section":"Docs","content":"Contact #  Feel free to contact me via: Lam Nguyen  Email: lamnguyen10@acm.org Social: Linkedin - Github Location: Hochiminh city, Vietnam   "},{"id":4,"href":"/docs/python/basic-operation-in-numpy/","title":"Basic matrix operation in numpy","section":"Python","content":"Basic matrix operation in numpy #  Introduction #  numpy is a great tool for performing matrix multiplication. Ever since changing from doing math on handwriting to using python, you often experience troubles finding the differences between matrix operations on numpy, and also how to map it to mathematic formula. This turorial helps to:\n Guide you over basic on how numpy array works. Understand three different operation of matrix operation in numpy: element-wise,matrix product, and dot product  How numpy array works #  In a general terms, numpy array can be interpreted as multidimensional array which all elements having the same data type. This seems not pretty much different compared to normal python array, but the reason behind is that numpy array is built on the purpose of efficient operation (loop, matrix operation, slicing, \u0026hellip;) which is better than python array in some cases, and numpy also has a great support for linear algebra, statistic operations which is suitable for data science jobs.\nLet first compare performace of python and numpy array over a sum of looping 10000 items.\n# sum 10000 item using python array py_array = range(0, 10000, 1) %timeit -n 10000 sum(py_array) 181 µs ± 6.05 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)  import numpy as np # sum 10000 items using numpy array np_array = np.arange(0, 10000, 1) %timeit -n 10000 np.sum(np_array) 7.26 µs ± 364 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)  You can see clearly that numpy array performs better than 180/7.28 = 24.725 times than python array in this case.\nNow, let\u0026rsquo;s also check another case where we should use python array instead:\n%%timeit def run_array(): response = [] for i in range(100000): response.append(i) return response run_array() 8.58 ms ± 477 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)  %%timeit import numpy as np # perf: numpy array def run_array(): response = np.empty((1, 0), int) for i in range(100000): response = np.append(response, np.array(i)) return response run_array() 2 s ± 106 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)  As you can see, the performance over the above trivial case is better in python array. Besides, when using numpy array you will also have to retain a fixed array of data type, while in python array you can store values of multiple data types (int, float, \u0026hellip;). In general:\n Using numpy array when you can vectorize your computation. Using python array when you have trivial tasks, check some condition, append some value, \u0026hellip;  Three different matrix operations in numpy #  numpy supports three different types of matrix operation:\n Element-wise product, denotes A * B Matrix product, denotes A @ B and matrix dot product, denotes A.dot(B)  Element-wise product #  This operation is also called Hadamard product, Schur product, entrywise product, and in mathematics, it\u0026rsquo;s denoted as $ \\circ $. This can be understood as same index multiplication, for example:\n$$ \\begin{bmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \u0026amp; \u0026hellip; \u0026amp; a_{1n} \\newline a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \u0026amp; \u0026hellip; \u0026amp; a_{2n} \\newline a_{n1} \u0026amp; a_{n2} \u0026amp; a_{n3} \u0026amp; \u0026hellip; \u0026amp; a_{nn} \\end{bmatrix} \\circ \\begin{bmatrix} b_{11} \u0026amp; b_{12} \u0026amp; b_{13} \u0026amp; \u0026hellip; \u0026amp; b_{1n} \\newline b_{21} \u0026amp; b_{22} \u0026amp; b_{23} \u0026amp; \u0026hellip; \u0026amp; b_{2n} \\newline b_{n1} \u0026amp; b_{n2} \u0026amp; b_{n3} \u0026amp; \u0026hellip; \u0026amp; b_{nn} \\end{bmatrix} = \\begin{bmatrix} a_{11}b_{11} \u0026amp; a_{12}b_{12} \u0026amp; a_{13}b_{13} \u0026amp; \u0026hellip; \u0026amp; a_{1n}b_{1n} \\newline a_{21}b_{21} \u0026amp; a_{22}b_{22} \u0026amp; a_{23}b_{23} \u0026amp; \u0026hellip; \u0026amp; a_{2n}b_{2n} \\newline \u0026hellip; \u0026amp; \u0026hellip; \u0026amp; \u0026hellip; \u0026amp; \u0026hellip; \u0026amp; \u0026hellip; \\newline a_{n1}b_{n1} \u0026amp; a_{n2}b_{n2} \u0026amp; a_{n3}b_{n3} \u0026amp; \u0026hellip; \u0026amp; a_{nn}b_{nn} \\end{bmatrix} $$\nLet do an example in numpy\nimport numpy as np A = np.array([ [1, 2, 3], [4, 5, 6] ]) B = np.array([ [3, 2, 1], [5, 4, 3] ]) C = A * B print(\u0026#34;Example C = A * B\u0026#34;) print(C) Example C = A * B [[ 3 4 3] [20 20 18]]  Matrix product and matrix dot product #  My experience with numpy was really confusing on how to map normal matrix multiplication which i use in my handwritting to an appropriate opration in numpy. Basically there are two options for it: matrix product @ and matrix dot product (.dot). The operation of those two is different as follows:\n In two dimensional array, both @ and .dot perform the same operation which is normal matrix multiplication. In higher dimensional array ($ n \u0026gt;= 3 $), they perform slightly different:  For @, it is treated as a stack of matrices residing in the last two indices and broadcast accordingly. For .dot, it is a sum product over the last axis of the first matrix and the second-to-last of the second matrix.    Let first take an example of 2 dimensional array to see they generate the same result as matrix multiplication\nimport numpy as np A = np.array([ [1, 2, 3], [4, 5, 6] ]) B = np.array([ [3, 2], [5, 4], [1, 3] ]) print(f\u0026#34;Example: @ operation of {A.shape}x {B.shape}\u0026#34;) C = A @ B print(f\u0026#34;A @ B equal \\n{C}\u0026#34;) print(f\u0026#34;.dot operation of {A.shape}and {B.shape}\u0026#34;) D = A.dot(B) print(f\u0026#34;A.dot(B) equal \\n{D}\u0026#34;) Example: @ operation of (2, 3) x (3, 2) A @ B equal [[16 19] [43 46]] Example: .dot operation of (2, 3) and (3, 2) A.dot(B) equal [[16 19] [43 46]]  The result is the same of the below two matrix multiplications:\n$$ \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\newline 4 \u0026amp; 5 \u0026amp; 6 \\end{bmatrix} \\begin{bmatrix} 3 \u0026amp; 2 \\newline 5 \u0026amp; 4 \\newline 1 \u0026amp; 3 \\end{bmatrix} = \\begin{bmatrix} 13 + 25 + 31 \u0026amp; 12 + 24 + 33 \\newline 43 + 55 + 61 \u0026amp; 42 + 5*4 + 6x3 \\end{bmatrix} = \\begin{bmatrix} 16 \u0026amp; 19 \\newline 43 \u0026amp; 46 \\end{bmatrix} $$\nAnd, Let take an example in 3 dimensonal array shape (2, 2, 3) and (2, 3, 2)\nimport numpy as np A = np.array([ [ [1, 2, 3], [4, 5, 6] ], [ [3, 3, 1], [1, 2, 4] ] ]) B = np.array([ [ [1, 2], [3, 4], [5, 6] ], [ [5, 6], [3, 4], [1, 2] ] ]) print(\u0026#34;-\u0026#34; * 50) print(f\u0026#34;Shape of matrix A = {A.shape}and B = {B.shape}\u0026#34;) print(\u0026#34;-\u0026#34; * 50) C = A @ B print(f\u0026#34;Result of A @ B equal \\n{C}with shape {C.shape}\u0026#34;) print(\u0026#34;-\u0026#34; * 50) D = A.dot(B) print(f\u0026#34;Result of A.dot(B) equal \\n{D}with shape {D.shape}\u0026#34;) print(\u0026#34;-\u0026#34; * 50) -------------------------------------------------- Shape of matrix A = (2, 2, 3) and B = (2, 3, 2) -------------------------------------------------- Result of A @ B equal [[[22 28] [49 64]] [[25 32] [15 22]]] with shape (2, 2, 2) -------------------------------------------------- Result of A.dot(B) equal [[[[22 28] [14 20]] [[49 64] [41 56]]] [[[17 24] [25 32]] [[27 34] [15 22]]]] with shape (2, 2, 2, 2) --------------------------------------------------  You can see that they\u0026rsquo;re generating different results. Turns out:\nIn @ case, it\u0026rsquo;s comprised of two matrix operation:\n$$ \\begin{bmatrix} 1 \u0026amp; 2 \u0026amp; 3 \\newline 4 \u0026amp; 5 \u0026amp; 6 \\end{bmatrix} \\begin{bmatrix} 1 \u0026amp; 2 \\newline 3 \u0026amp; 4 \\newline 5 \u0026amp; 6 \\end{bmatrix} = \\begin{bmatrix} 22 \u0026amp; 28 \\newline 49 \u0026amp; 64 \\end{bmatrix} $$\nand\n$$ \\begin{bmatrix} 3 \u0026amp; 3 \u0026amp; 1 \\newline 1 \u0026amp; 2 \u0026amp; 4 \\end{bmatrix} \\begin{bmatrix} 5 \u0026amp; 6 \\newline 3 \u0026amp; 4 \\newline 1 \u0026amp; 2 \\end{bmatrix} = \\begin{bmatrix} 25 \u0026amp; 32 \\newline 15 \u0026amp; 22 \\end{bmatrix} $$\nIn .dot case, the operation acts like nested looping over dimensions that are greater than 2 and performs vector product from each associated row and column for last second dimensions. The below is a simulation on how this operation would work in 3 dimensional array.\nmatrix_result = np.empty([2, 2, 2, 2]) for matrix_A_id, matrix_A in enumerate(A): for row_A_id, row_A in enumerate(matrix_A): for matrix_B_id, matrix_B in enumerate(B): for col_B_id, col_B in enumerate(matrix_B.T): matrix_result[matrix_A_id, matrix_B_id, row_A_id, col_B_id ] = row_A * col_B References #   Numpy quickstart Python Lists vs. Numpy Arrays - What is the difference?  "},{"id":5,"href":"/docs/math/derivative-of-matrix/","title":"Derivative of inverse, eigenvalues and singular values","section":"Math","content":"Derivative of inverse, eigenvalues and singular values #  Introduction #  Question: Given an invertible matrix $ A $, how can we find derivatives of:\n $ A^{-1} $: which denotes inverse matrix of $ A $ $ \\lambda $: which denotes eigenvalues of $ A $ $ \\sigma $: which denote singular values of $ A $  Let\u0026rsquo;s start to find the formula for each case.\nFinding derivative of inverse matrix #  Problem: Given matrix $ A $ subjected to variable $ t $: $ \\frac{dA}{dt} $, finding $ \\frac{A^{-1}}{dt} $\nWith two invertible matrices $ A, B $, we have\n$ B^{-1} - A^{-1} = B^{-1}(A - B)A^{-1} $\nWe can pick any abitrary invertible matrix $ B $. Let takes $ B = A + \\nabla A $ and place in the above formula, we have:\n$$ \\begin{align*} \u0026amp; (A + \\nabla A)^{-1} - A^{-1} = (A + \\nabla A)^{-1}(A - B)A^{-1} \\newline \\leftrightarrow \\quad \u0026amp; \\frac{\\nabla A^{-1}}{\\nabla t} = -(A + \\nabla A)^{-1}(\\frac{\\nabla A}{\\nabla t})A^{-1} \\end{align*} $$\nAnd as $ \\nabla t \\to 0 $, we have:\n$$ \\begin{align*} \u0026amp; \\lim_{\\nabla t \\to 0 \\leftrightarrow \\nabla A \\to 0 } \\frac{\\nabla A^{-1}}{\\nabla t} = \\lim_{\\nabla t \\to 0 \\leftrightarrow \\nabla A \\to 0 } - (A + \\nabla A)^{-1}(\\frac{\\nabla A}{\\nabla t})A^{-1} \\newline \\leftrightarrow \\quad \u0026amp; \\lim_{\\nabla t \\to 0 \\leftrightarrow \\nabla A \\to 0 } \\frac{\\nabla A^{-1}}{\\nabla t} = \\lim_{\\nabla t \\to 0} - (A)^{-1}(\\frac{\\nabla A}{\\nabla t})A^{-1} \\end{align*} $$\nThis is equivalent to definition of derivative of matrix $ A^{-1} $. So the final answer is:\n$$ \\frac{d A^{-1}}{dt} = - A^{-1}\\frac{dA}{dt}A^{-1} $$\nDerivative of eigenvalue $ \\lambda $ #  Problem: Given matrix $ A $ subjected to variable $ t $, find $ \\frac{d \\lambda }{dt} $, where $ \\lambda $ is eigenvalue of matrix $ A $.\nRecalling there are two type of eigenvectors: left and right. The below table shows formula on relationship between eigenvalue $ \\lambda $, eigenvector $ x $ and the original matrix $ A $\n   Form Left Right     vector $ y^TA = \\lambda y^T $ $ Ax = \\lambda x $   matrix $ Y^TA = \\Lambda Y^T $ $ AX = X \\Lambda $    And: $ Y^TX = I $\nUsing those formula with a function of $ t $, we have:\n$$ \\begin{align*} \u0026amp; y^T(t) A(t) x(t) = \\lambda (t) y^T(t) x(t) \\newline \\leftrightarrow \\quad \u0026amp; \\lambda (t) = y^T(t) A(t) x(t) \\end{align*} $$\nSo calulate derivative, with note that $ y^T(t)x(t) = I $\n$$ \\begin{align*} \\frac{d \\lambda}{dt} \u0026amp; = \\frac{d y^T}{dt} A x(t) + y^T(t) \\frac{d A} {dt} x(t) + y^T(t) A(t) \\frac{d x}{dt} \\newline \u0026amp; = (\\frac{d y^T}{dt} A x(t) + y^T(t) A(t) \\frac{d x}{dt} ) + y^T(t) \\frac{d A} {dt} x(t) \\newline \u0026amp; = \\lambda (t) (\\frac{d y^T}{dt} x(t) + y^T(t)\\frac{d x}{dt}) + y^T(t) \\frac{d A} {dt} x(t) \\newline \u0026amp; = 0 + y^T(t) \\frac{d A} {dt} x(t) \\newline \u0026amp; = y^T(t) \\frac{d A} {dt} x(t) \\newline \\end{align*} $$\nSo the final answer is:\n$$ \\frac{d \\lambda}{dt} = y^T(t)\\frac{dA}{dt}x(t) $$\nDerivative of $ \\sigma $ #  Problem: Given matrix $ A $ subjected to variable $ t $, finding $ \\frac{d(\\sigma)}{d(t)} $, which denotes singular values of matrix $ A $.\nRecall that $ A $ can be written as SVD form as $ A = U^T \\Sigma V $, where $ U^T $, $ V $ are orthogonal matrices, and $ \\Sigma $ is symmetric matrix which contains singular values in diagonal line.\n$$ \\begin{align*} A = U \\Sigma V^T \\leftrightarrow \\Sigma = U^TAV \\end{align*} $$\nWith a any $ \\sigma $ in $ \\Sigma $, we have:\n$$ \\begin{align*} \\frac{d \\sigma}{dt} \u0026amp; = \\frac{d(u^TAv)}{dt} \\newline \u0026amp; = \\frac{du^T}{dt}Av + u^T\\frac{dA}{dt}v + u^TA\\frac{dv}{dt} \\newline \u0026amp; = \\frac{du^T}{df}u\\sigma + u^T\\frac{dA}{dt}v + \\sigma v^T\\frac{dv}{dt} \\end{align*} $$\nRecall that $ u^Tu = 0 $ and $ v^Tv = 0 $ since $ u, v $ are orthogonal. Therefore:\n$$ \\begin{align*} \\frac{d \\sigma}{dt} \u0026amp; = \\frac{du^T}{df}u\\sigma + u^T\\frac{dA}{dt}v + \\sigma v^T\\frac{dv}{dt} \\newline \u0026amp; = 0 + u^T\\frac{dA}{dt}v + 0 \\newline \u0026amp; = u^T\\frac{dA}{dt}v \\end{align*} $$\nSo the final answer is:\n$$ \\frac{d \\sigma}{dt} = u^T\\frac{dA}{dt}v $$\n"}]