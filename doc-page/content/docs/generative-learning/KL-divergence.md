# Kullbackâ€“Leibler Divergence

## Definition

`Kullback-Leibler Divergence` is the measurement of logarithm of difference between two distributions.

$
D_{KL} ( P || Q) = \sum_{x \in X} P(x) \log{\frac{P(x)}{Q(x)}}
$

It can also be understood using [entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory))

Reference: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence